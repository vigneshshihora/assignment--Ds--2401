{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c9e14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon(product):\n",
    "   \n",
    "    search_query = product.replace(' ', '+')\n",
    "    url = f'https://www.amazon.in/s?k={search_query}'\n",
    "\n",
    "   \n",
    "    response = requests.get(url)\n",
    "\n",
    "   \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "   \n",
    "    product_titles = soup.find_all('span', class_='a-text-normal')\n",
    "\n",
    "    \n",
    "    if product_titles:\n",
    "        print(\"Products found for your search:\")\n",
    "        for title in product_titles:\n",
    "            print(title.text)\n",
    "    else:\n",
    "        print(\"No products found for your search.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product you want to search on Amazon India: \")\n",
    "    search_amazon(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b851e244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_product_details(product_url):\n",
    "    details = {\n",
    "        \"Brand Name\": \"-\",\n",
    "        \"Name of the Product\": \"-\",\n",
    "        \"Price\": \"-\",\n",
    "        \"Return/Exchange\": \"-\",\n",
    "        \"Expected Delivery\": \"-\",\n",
    "        \"Availability\": \"-\",\n",
    "        \"Product URL\": product_url\n",
    "    }\n",
    "\n",
    "    response = requests.get(product_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    brand = soup.find('span', id='productTitle')\n",
    "    if brand:\n",
    "        details[\"Brand Name\"] = brand.text.strip().split(' ')[0]\n",
    "\n",
    "    name = soup.find('span', id='productTitle')\n",
    "    if name:\n",
    "        details[\"Name of the Product\"] = name.text.strip()\n",
    "\n",
    "    price = soup.find('span', id='priceblock_ourprice')\n",
    "    if price:\n",
    "        details[\"Price\"] = price.text.strip()\n",
    "\n",
    "    return_exchange = soup.find('div', {'data-name': 'RETURNS_POLICY'})\n",
    "    if return_exchange:\n",
    "        details[\"Return/Exchange\"] = return_exchange.text.strip()\n",
    "\n",
    "    expected_delivery = soup.find('div', {'data-name': 'ESTIMATED_DELIVERY_DATE'})\n",
    "    if expected_delivery:\n",
    "        details[\"Expected Delivery\"] = expected_delivery.text.strip()\n",
    "\n",
    "    availability = soup.find('span', {'class': 'a-size-medium a-color-success'})\n",
    "    if availability:\n",
    "        details[\"Availability\"] = availability.text.strip()\n",
    "\n",
    "    return details\n",
    "\n",
    "def scrape_amazon_products(product):\n",
    "    search_query = product.replace(' ', '+')\n",
    "    base_url = f'https://www.amazon.in/s?k={search_query}'\n",
    "\n",
    "    all_products = []\n",
    "\n",
    "    for page_num in range(1, 4):  # Scraping first 3 pages\n",
    "        url = f'{base_url}&page={page_num}'\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        product_links = soup.find_all('a', class_='a-link-normal.a-text-normal')\n",
    "        for link in product_links:\n",
    "            product_url = 'https://www.amazon.in' + link.get('href')\n",
    "            product_details = get_product_details(product_url)\n",
    "            all_products.append(product_details)\n",
    "\n",
    "    df = pd.DataFrame(all_products)\n",
    "    df.to_csv('amazon_products.csv', index=False)\n",
    "    print(\"Data has been scraped and saved to 'amazon_products.csv'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product you want to search on Amazon India: \")\n",
    "    scrape_amazon_products(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5148cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def scrape_images(keyword, num_images=10):\n",
    "    # Initialize Selenium WebDriver (Make sure you have chromedriver installed)\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Open Google Images\n",
    "    driver.get(\"https://www.google.com/imghp?hl=en\")\n",
    "\n",
    "    # Find the search bar and enter the keyword\n",
    "    search_bar = driver.find_element_by_name(\"q\")\n",
    "    search_bar.clear()\n",
    "    search_bar.send_keys(keyword)\n",
    "    search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Scroll down to load more images\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Get the page source after scrolling\n",
    "    page_source = driver.page_source\n",
    "    driver.quit()\n",
    "\n",
    "    # Parse the page source with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Extract image URLs\n",
    "    image_tags = soup.find_all('img', class_='rg_i')\n",
    "    image_urls = [tag['src'] for tag in image_tags[:num_images]]\n",
    "\n",
    "    return image_urls\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "    for keyword in keywords:\n",
    "        print(f\"Scraping images for: {keyword}\")\n",
    "        images = scrape_images(keyword, num_images=10)\n",
    "        print(images)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faa4b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_smartphones(search_query):\n",
    "    # Format the search query\n",
    "    search_query = search_query.replace(' ', '+')\n",
    "    url = f'https://www.flipkart.com/search?q={search_query}&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off'\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the smartphone listings\n",
    "    smartphone_listings = soup.find_all('div', class_='_1AtVbE')\n",
    "\n",
    "    # List to store all smartphone details\n",
    "    smartphones_data = []\n",
    "\n",
    "    for listing in smartphone_listings:\n",
    "        details = {\n",
    "            \"Brand Name\": \"-\",\n",
    "            \"Smartphone Name\": \"-\",\n",
    "            \"Colour\": \"-\",\n",
    "            \"RAM\": \"-\",\n",
    "            \"Storage(ROM)\": \"-\",\n",
    "            \"Primary Camera\": \"-\",\n",
    "            \"Secondary Camera\": \"-\",\n",
    "            \"Display Size\": \"-\",\n",
    "            \"Battery Capacity\": \"-\",\n",
    "            \"Price\": \"-\",\n",
    "            \"Product URL\": \"-\"\n",
    "        }\n",
    "\n",
    "        # Extracting details from the listing\n",
    "        brand = listing.find('div', class_='_4rR01T').text\n",
    "        details[\"Brand Name\"] = brand.split(' ')[0]  # Extracting brand name\n",
    "        details[\"Smartphone Name\"] = brand  # Setting smartphone name\n",
    "        details[\"Price\"] = listing.find('div', class_='_30jeq3 _1_WHN1').text.strip('â‚¹')  # Extracting price\n",
    "\n",
    "        # Extracting other details if available\n",
    "        features = listing.find_all('li', class_='rgWa7D')\n",
    "        for feature in features:\n",
    "            feature_text = feature.text\n",
    "            if \"RAM\" in feature_text:\n",
    "                details[\"RAM\"] = feature_text.split(' ')[0]\n",
    "            elif \"ROM\" in feature_text:\n",
    "                details[\"Storage(ROM)\"] = feature_text.split(' ')[0]\n",
    "            elif \"MP\" in feature_text:\n",
    "                if \"Primary\" in feature_text:\n",
    "                    details[\"Primary Camera\"] = feature_text\n",
    "                elif \"Secondary\" in feature_text:\n",
    "                    details[\"Secondary Camera\"] = feature_text\n",
    "            elif \"Display Size\" in feature_text:\n",
    "                details[\"Display Size\"] = feature_text\n",
    "            elif \"Battery Capacity\" in feature_text:\n",
    "                details[\"Battery Capacity\"] = feature_text\n",
    "            elif \"Color\" in feature_text:\n",
    "                details[\"Colour\"] = feature_text\n",
    "\n",
    "        # Extracting product URL\n",
    "        product_url = 'https://www.flipkart.com' + listing.find('a', class_='IRpwTa')['href']\n",
    "        details[\"Product URL\"] = product_url\n",
    "\n",
    "        smartphones_data.append(details)\n",
    "\n",
    "    return smartphones_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_query = input(\"Enter the smartphone you want to search on Flipkart: \")\n",
    "    smartphones_data = scrape_smartphones(search_query)\n",
    "\n",
    "    # Creating a DataFrame from the scraped data\n",
    "    df = pd.DataFrame(smartphones_data)\n",
    "\n",
    "    # Saving the DataFrame to a CSV file\n",
    "    df.to_csv('flipkart_smartphones.csv', index=False)\n",
    "    print(\"Data has been scraped and saved to 'flipkart_smartphones.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d024091",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install googlemaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91084dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_coordinates(city_name):\n",
    "    # Google Maps Geocoding API endpoint\n",
    "    url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
    "\n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        \"address\": city_name,\n",
    "        \"key\": \"AIzaSyDfe1rvtH3w5RcyJXSAE-4LOANU1FekeoU\"  # Replace 'YOUR_API_KEY' with your actual Google Maps API key\n",
    "    }\n",
    "\n",
    "    # Send a GET request to the API\n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "        \n",
    "        # Check if the response contains any results\n",
    "        if data['status'] == 'OK' and len(data['results']) > 0:\n",
    "            # Extract latitude and longitude from the first result\n",
    "            location = data['results'][0]['geometry']['location']\n",
    "            latitude = location['lat']\n",
    "            longitude = location['lng']\n",
    "            return latitude, longitude\n",
    "        else:\n",
    "            print(\"No results found for the city.\")\n",
    "            return None, None\n",
    "    else:\n",
    "        print(\"Error occurred while accessing the API.\")\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    city_name = input(\"Enter the name of the city: \")\n",
    "    latitude, longitude = get_coordinates(city_name)\n",
    "    if latitude is not None and longitude is not None:\n",
    "        print(f\"The coordinates of {city_name} are: Latitude: {latitude}, Longitude: {longitude}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec34f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_gaming_laptops():\n",
    "    url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all the laptop listings\n",
    "    laptop_listings = soup.find_all('div', class_='TopNumbeHeading active sticky-footer')\n",
    "\n",
    "    # List to store all laptop details\n",
    "    laptops_data = []\n",
    "\n",
    "    for listing in laptop_listings:\n",
    "        details = {\n",
    "            \"Model\": \"-\",\n",
    "            \"Price\": \"-\",\n",
    "            \"Specifications\": \"-\"\n",
    "        }\n",
    "\n",
    "        # Extracting details from the listing\n",
    "        model = listing.find('div', class_='TopNumbeHeading sticky-footer').text.strip()\n",
    "        details[\"Model\"] = model\n",
    "\n",
    "        price = listing.find('div', class_='SectionPrice').text.strip()\n",
    "        details[\"Price\"] = price\n",
    "\n",
    "        specifications = listing.find('div', class_='SpecsDetail').text.strip()\n",
    "        details[\"Specifications\"] = specifications\n",
    "\n",
    "        laptops_data.append(details)\n",
    "\n",
    "    return laptops_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    laptops_data = scrape_gaming_laptops()\n",
    "\n",
    "    # Creating a DataFrame from the scraped data\n",
    "    df = pd.DataFrame(laptops_data)\n",
    "\n",
    "    # Saving the DataFrame to a CSV file\n",
    "    df.to_csv('gaming_laptops_digit.csv', index=False)\n",
    "    print(\"Data has been scraped and saved to 'gaming_laptops_digit.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e122b41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_billionaires():\n",
    "    url = \"https://www.forbes.com/billionaires/\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the table containing billionaire details\n",
    "    table = soup.find('table', class_='table')\n",
    "\n",
    "    # Find all rows in the table\n",
    "    rows = table.find_all('tr')[1:]  # Exclude the header row\n",
    "\n",
    "    # List to store all billionaire details\n",
    "    billionaires_data = []\n",
    "\n",
    "    for row in rows:\n",
    "        details = {\n",
    "            \"Rank\": \"-\",\n",
    "            \"Name\": \"-\",\n",
    "            \"Net Worth\": \"-\",\n",
    "            \"Age\": \"-\",\n",
    "            \"Citizenship\": \"-\",\n",
    "            \"Source\": \"-\",\n",
    "            \"Industry\": \"-\"\n",
    "        }\n",
    "\n",
    "        # Extracting details from the row\n",
    "        cells = row.find_all('td')\n",
    "        details[\"Rank\"] = cells[0].text.strip()\n",
    "        details[\"Name\"] = cells[1].text.strip()\n",
    "        details[\"Net Worth\"] = cells[2].text.strip()\n",
    "        details[\"Age\"] = cells[3].text.strip()\n",
    "        details[\"Citizenship\"] = cells[4].text.strip()\n",
    "        details[\"Source\"] = cells[5].text.strip()\n",
    "        details[\"Industry\"] = cells[6].text.strip()\n",
    "\n",
    "        billionaires_data.append(details)\n",
    "\n",
    "    return billionaires_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    billionaires_data = scrape_billionaires()\n",
    "\n",
    "    # Creating a DataFrame from the scraped data\n",
    "    df = pd.DataFrame(billionaires_data)\n",
    "\n",
    "    # Saving the DataFrame to a CSV file\n",
    "    df.to_csv('forbes_billionaires.csv', index=False)\n",
    "    print(\"Data has been scraped and saved to 'forbes_billionaires.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2881c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "\n",
    "def get_video_comments(video_id, api_key, max_comments=500):\n",
    "    # Initialize the YouTube API client\n",
    "    youtube = build('youtube', 'v3', developerKey=VQQ6uiXDlGo&t=554s)\n",
    "\n",
    "    # List to store comments data\n",
    "    comments_data = []\n",
    "\n",
    "    # Request the video comments\n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        videoId=video_id,\n",
    "        maxResults=100,  # Maximum 100 comments per request\n",
    "        order=\"time\"  # Order comments by time\n",
    "    )\n",
    "\n",
    "    # Fetch comments until reaching the specified limit or no more comments available\n",
    "    total_comments = 0\n",
    "    while request and total_comments < max_comments:\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response[\"items\"]:\n",
    "            comment = item[\"snippet\"][\"topLevelComment\"]\n",
    "            text = comment[\"snippet\"][\"textDisplay\"]\n",
    "            likes = comment[\"snippet\"][\"likeCount\"]\n",
    "            time = comment[\"snippet\"][\"publishedAt\"]\n",
    "\n",
    "            comments_data.append({\n",
    "                \"Comment\": text,\n",
    "                \"Upvotes\": likes,\n",
    "                \"Time\": time\n",
    "            })\n",
    "\n",
    "            total_comments += 1\n",
    "            if total_comments >= max_comments:\n",
    "                break\n",
    "\n",
    "        if \"nextPageToken\" in response:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                pageToken=response[\"nextPageToken\"],\n",
    "                maxResults=100,\n",
    "                order=\"time\"\n",
    "            )\n",
    "        else:\n",
    "            request = None\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Provide the YouTube video ID and your API key\n",
    "    video_id = input(\"Enter the YouTube video ID: \")\n",
    "    api_key = input(\"Enter your YouTube Data API key: \")\n",
    "\n",
    "    # Fetch comments data\n",
    "    comments_data = get_video_comments(video_id, api_key, max_comments=500)\n",
    "\n",
    "    # Creating a DataFrame from the comments data\n",
    "    df = pd.DataFrame(comments_data)\n",
    "\n",
    "    # Saving the DataFrame to a CSV file\n",
    "    df.to_csv('youtube_comments.csv', index=False)\n",
    "    print(\"Comments data has been saved to 'youtube_comments.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdda8078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_hostels_in_london():\n",
    "    url = \"https://www.hostelworld.com/hostels/London\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all hostel listings\n",
    "    hostel_listings = soup.find_all('div', class_='fab1ib-2')\n",
    "\n",
    "    # List to store all hostel details\n",
    "    hostels_data = []\n",
    "\n",
    "    for hostel in hostel_listings:\n",
    "        details = {\n",
    "            \"Hostel Name\": \"-\",\n",
    "            \"Distance from City Centre\": \"-\",\n",
    "            \"Ratings\": \"-\",\n",
    "            \"Total Reviews\": \"-\",\n",
    "            \"Overall Reviews\": \"-\",\n",
    "            \"Privates from Price\": \"-\",\n",
    "            \"Dorms from Price\": \"-\",\n",
    "            \"Facilities\": \"-\",\n",
    "            \"Property Description\": \"-\"\n",
    "        }\n",
    "\n",
    "        # Extracting details from the listing\n",
    "        details[\"Hostel Name\"] = hostel.find('h2', class_='title-2').text.strip()\n",
    "\n",
    "        distance = hostel.find('span', class_='description').text.strip()\n",
    "        details[\"Distance from City Centre\"] = distance.split(' ')[0]\n",
    "\n",
    "        rating = hostel.find('div', class_='score orange big').text.strip()\n",
    "        details[\"Ratings\"] = rating\n",
    "\n",
    "        reviews = hostel.find('div', class_='reviews').text.strip()\n",
    "        details[\"Total Reviews\"] = reviews.split()[0]\n",
    "        details[\"Overall Reviews\"] = reviews.split()[-2]\n",
    "\n",
    "        price_tags = hostel.find_all('div', class_='price-col')\n",
    "        details[\"Privates from Price\"] = price_tags[0].text.strip().split('\\n')[1]\n",
    "        details[\"Dorms from Price\"] = price_tags[1].text.strip().split('\\n')[1]\n",
    "\n",
    "        facilities = [facility.text.strip() for facility in hostel.find_all('li', class_='facility-badge')]\n",
    "        details[\"Facilities\"] = ', '.join(facilities)\n",
    "\n",
    "        description = hostel.find('div', class_='rating-factors prop-card-tablet')\n",
    "        if description:\n",
    "            details[\"Property Description\"] = description.text.strip().replace('\\n', '')\n",
    "\n",
    "        hostels_data.append(details)\n",
    "\n",
    "    return hostels_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hostels_data = scrape_hostels_in_london()\n",
    "\n",
    "    # Creating a DataFrame from the scraped data\n",
    "    df = pd.DataFrame(hostels_data)\n",
    "\n",
    "    # Saving the DataFrame to a CSV file\n",
    "    df.to_csv('hostels_in_london.csv', index=False)\n",
    "    print(\"Data has been scraped and saved to 'hostels_in_london.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0aef72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed06c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf848f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
